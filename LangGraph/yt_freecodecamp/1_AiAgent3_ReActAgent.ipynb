{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0280f03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from dotenv import load_dotenv  \n",
    "from langchain_core.messages import BaseMessage # The foundational class for all message types in LangGraph\n",
    "from langchain_core.messages import ToolMessage # Passes data back to LLM after it calls a tool such as the content and the tool_call_id\n",
    "from langchain_core.messages import SystemMessage # Message for providing instructions to the LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41116c",
   "metadata": {},
   "source": [
    "#### Pre-requisite : Meaning of different imports done above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45abdc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This has to follow abc@xyz.com format',)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Meaning of different imports done above \"\"\"\n",
    "\n",
    "# 1) Annotated: provides additional context without affecting the type itself\n",
    "email = Annotated[str, \"This has to follow abc@xyz.com format\"]; # -> so this says email will be a string with additionl metadata attached as \"This has to follow abc@xyz.com format\"\n",
    "\n",
    "print(email.__metadata__)\n",
    "\n",
    "# 2) Sequence: To automatically handle the state updates for sequences such as by adding new messages to a chat history\n",
    "\n",
    "'''\n",
    "  3) Reducer function (add_messages in above imports): a rulde that controls how updates from nodes are combined with the existing state.\n",
    "  \n",
    "  Simply put: it tells us how to merge new data into the current state\n",
    "'''\n",
    "\n",
    "# Without Reducer\n",
    "old_state = {\"messages\": [\"HI!\"]}\n",
    "update_from_node = {\"messages\": [\"Nice to meet you!\"]}\n",
    "new_state = {\"messages\": [\"Nice to meet you!\"]}\n",
    "\n",
    "# With reducer\n",
    "old_state = {\"messages\": [\"HI!\"]}\n",
    "update_from_node = {\"messages\": [\"Nice to meet you!\"]}\n",
    "new_state = {\"messages\": [\"Hi!\", \"Nice to meet you!\"]}\n",
    "\n",
    "# Note: for simpler applications, we can use .append() method to append new messages to the \"messages\" list but it won't work when we actually introduce cmoplexity (like LLM with tools) because a reducer does more than just .append(), like correct format, adding ids and sometimes the framework (here, langgraph) expects you to just return the updates not directly mutate the state (compare this to react which also expects you to use the state update function and not modify the state directly)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ad2be",
   "metadata": {},
   "source": [
    "#### Now, the ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d46897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages] # This means Datatype: \"Sequence[BaseMessage]\"; metadata = \"add_message\"\n",
    "\n",
    "@tool\n",
    "def add(a:int, b:int):\n",
    "    \"\"\"This is an addition function that adds 2 numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int):\n",
    "    \"\"\"Subtraction function\"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int):\n",
    "    \"\"\"Multiplication function\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, subtract, multiply]\n",
    "model = init_chat_model(\"google_genai:gemini-2.0-flash\").bind_tools(tools)\n",
    "\n",
    "def model_call(state:AgentState) -> AgentState:\n",
    "    system_prompt = SystemMessage(content=\"You are my AI assistant, answer my questions to the best of your ability.\")\n",
    "    response = model.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state:AgentState) -> str:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # In LangChain and LangGraph, the \".tool_calls\" attribute is found on AIMessage objects and represents a list of tool calls that a language model (LLM) has decided to make in response to a prompt. \n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "    \n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"our_agent\", model_call)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph.add_edge(START, \"our_agent\")\n",
    "graph.add_conditional_edges(\n",
    "    \"our_agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"continue\": \"tools\"\n",
    "    },\n",
    ")\n",
    "\n",
    "graph.add_edge(\"tools\", \"our_agent\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db77e585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 40 + 12. Add 2 + 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (90e8247a-6316-4c57-86ec-527842d79435)\n",
      " Call ID: 90e8247a-6316-4c57-86ec-527842d79435\n",
      "  Args:\n",
      "    a: 40.0\n",
      "    b: 12.0\n",
      "  add (28f0ea11-2c29-490c-9d70-bbc6b4f653ed)\n",
      " Call ID: 28f0ea11-2c29-490c-9d70-bbc6b4f653ed\n",
      "  Args:\n",
      "    a: 2.0\n",
      "    b: 3.0\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "5\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The answer to 40 + 12 is 52 and the answer to 2 + 3 is 5.\n"
     ]
    }
   ],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"Add 40 + 12. Add 2 + 3\")]}\n",
    "print_stream(app.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b435d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 40 + 12 and then multiply the result by 6. Also tell me a joke please.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (1a3b2d3b-ecb3-450c-a7ad-888139c1f795)\n",
      " Call ID: 1a3b2d3b-ecb3-450c-a7ad-888139c1f795\n",
      "  Args:\n",
      "    a: 40.0\n",
      "    b: 12.0\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "52\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (033eb1a5-4160-482e-84c8-ed847c0daa4d)\n",
      " Call ID: 033eb1a5-4160-482e-84c8-ed847c0daa4d\n",
      "  Args:\n",
      "    a: 52.0\n",
      "    b: 6.0\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "312\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The answer is 312.\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"Add 40 + 12 and then multiply the result by 6. Also tell me a joke please.\")]}\n",
    "print_stream(app.stream(inputs, stream_mode=\"values\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
