{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c318428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()  # take environment variables from .env file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa9eba",
   "metadata": {},
   "source": [
    "### Using Google 2.5 flash with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d83a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59194cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--4d55d1c3-0073-4aad-86be-b74bd99ae5e3-0', usage_metadata={'input_tokens': 7, 'output_tokens': 367, 'total_tokens': 374, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 356}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello, who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf4438",
   "metadata": {},
   "source": [
    "### Making the graph using Google 2.5 flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba994a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "def chat_bot(state: State) -> State:\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    state[\"messages\"].append({\"role\": \"ai\", \"content\": response.content})\n",
    "    return state\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Adding nodes to the graph\n",
    "builder.add_node(\"chat_bot_node\", chat_bot) # Syntax: \"<name of node>\", <function that the node will execute>\n",
    "\n",
    "\n",
    "# Adding edges to the graph\n",
    "builder.add_edge(START, \"chat_bot_node\") # Syntax: <from node>, <to node>; START and END are special nodes. More specifically, the START Node is a special node that \n",
    "builder.add_edge(\"chat_bot_node\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a656149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:  1 + 1 = 2\n",
      "AI Response:  2 + 2 = 4\n",
      "AI Response:  So far, you've asked me:\n",
      "\n",
      "1.  \"1+1?\" (My answer was \"1 + 1 = 2\")\n",
      "2.  \"2+2\" (My answer was \"2 + 2 = 4\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Format of messages for Gemini 2.5 flash to respond appropriately with context:\n",
    "\n",
    "    messages = [{\"role\":\"user\", \"content\":\"What is the capital of India?\"},\n",
    "           {\"role\":\"ai\", \"content\":\"Delhi is the capital of India.\"},\n",
    "           {\"role\":\"user\", \"content\":\"And that of France?\"}\n",
    "           ]\n",
    "\"\"\"\n",
    "\n",
    "import stat\n",
    "\n",
    "\n",
    "state: State = {\"messages\": [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"}]}\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Ask Anything: \")\n",
    "    if(user_input.lower() in [\"exit\", \"quit\", \"q\"]):\n",
    "        break\n",
    "\n",
    "    state[\"messages\"].append({\"role\":\"user\", \"content\":user_input})\n",
    "    graph.invoke(state)  # To initialize the graph with the system message\n",
    "    print(\"AI Response: \", state[\"messages\"][-1][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
